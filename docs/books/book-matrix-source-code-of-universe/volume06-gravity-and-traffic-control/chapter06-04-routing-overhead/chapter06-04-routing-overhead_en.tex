\section{Routing Overhead}

\textbf{--- Why Archived Data Still Clogs the Network?}

\textbf{``The server isn't running that large file, but just indexing its location exhausts the router's computational power.''}

---

\subsection{Static Load vs. Dynamic Load}

In the previous chapter, we reconstructed black holes as the universe's \textbf{``cold storage''}. This means that matter evolution inside black holes has stopped (\textbf{$v_{int} \to 0$}), becoming static data. This raises a critical architectural question: Since black holes have stopped running any logic internally and don't consume $v_{int}$, why do they still have a massive impact on the external network (gravitational lensing, Shapiro delay)? Doesn't this require computational power?

In the FS-QCA architecture, we need to distinguish between two types of computational consumption:

\begin{enumerate}
\item \textbf{Object Cost ($v_{int}$):}

    This is the cost of an object maintaining its own state updates. For black holes, this cost is indeed frozen. Black holes themselves don't ``think'' or ``age.''

\item \textbf{Topological Overhead (Metric Cost):}

    This is the maintenance cost that the \textbf{spacetime grid (QCA Grid)} must pay to accommodate this high-density object. Although black holes are ``dead,'' they are massive \textbf{topological defects}. To embed this huge data block in the discrete grid, surrounding grid nodes must rearrange their connection relationships (entanglement structure), causing extremely high \textbf{connection density} in that region.
\end{enumerate}

\subsection{Why Does Light Slow Down?}

When light passes near a black hole, \textbf{Shapiro delay} indeed occurs---light appears to slow down. In our architecture, photons (data packets) don't actually slow down; their local hop speed remains \textbf{$v_{LR}$} (the speed of light).

The slowdown is due to increased \textbf{hop count}.

\textbf{Mechanism Analysis: Spatial Inflation}

\begin{itemize}
\item \textbf{High-Density Nodes:} The QCA grid around a black hole must increase local node density or entanglement complexity to carry the massive gravitational flux (information flow). This corresponds to the spatial metric component \textbf{$g_{rr} > 1$} in general relativity.

\item \textbf{Longer Paths:} In flat space, going from node A to node B might require only 100 hops. But near a black hole, because space is ``stretched'' (actually, more entangled nodes are inserted to maintain connections), the shortest path (geodesic) from A to B might become 150 hops.

\item \textbf{Result:} Although photons travel at the same speed each step, they need to take more steps. The delay time \textbf{$T_{delay}$} is:

    $$T_{delay} = (N_{curved} - N_{flat}) \times \Delta \tau$$
\end{itemize}

\textbf{Conclusion:}

Black holes don't consume ``evolutionary computational power,'' but rather \textbf{``routing computational power''}. They force all signals passing nearby to take detours (or more accurately, traverse more grid hops). This path extension is perceived macroscopically as light slowing down or time delay.

\subsection{A System Metaphor}

Imagine a \textbf{giant database table (Black Hole)} filled with data.

\begin{itemize}
\item This table is \textbf{read-only/archived}; no one is modifying it (\textbf{$v_{int}=0$}).

\item However, because this table is so large (high mass), it occupies a huge \textbf{index space}.

\item When a \textbf{query request (photon)} tries to pass through this database, although it doesn't read the table's contents, the database engine (spacetime geometry) must scan the massive index tree to determine the path.

\item \textbf{Result:} The query slows down. Not because the data is moving, but because \textbf{the existence of the data itself distorts the addressing space}.
\end{itemize}

---

\section{The Architect's Note}

\subsection{About: Metadata Overhead}

In distributed storage systems, we must not only store the data itself, but also maintain \textbf{metadata}---such as data block locations, checksums, and access permissions.

\begin{itemize}
\item \textbf{Gravitational Field is Metadata:}

    What's stored inside a black hole is the actual \textbf{Payload}.

    The curved spacetime outside a black hole is the \textbf{metadata layer} that the system maintains to manage this Payload.

\item \textbf{Where Does the Computational Power Go?}

    You ask, ``Black holes still consume computational resources, right?'' Yes, the resources they consume manifest in \textbf{vacuum polarization}. To maintain the curved geometric structure around a black hole, the vacuum ground state must maintain a high-energy entanglement configuration. This is like how a file system must continuously occupy part of memory to cache its \textbf{Inode table} to maintain a huge static file.
\end{itemize}

So, light slows down because it's traversing a region with \textbf{extremely dense metadata}. It gets caught in heavy ``addressing computations.'' Gravity is not a force; it's the \textbf{administrative overhead} generated when the system maintains massive data indices.

